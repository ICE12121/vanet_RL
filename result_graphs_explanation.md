# VANET RL Simulation: Result Graphs & Baseline Strategies Explained

This document explains each result graph generated by the simulation and clarifies the meaning of the **Random** and **Greedy** baseline strategies in both training and testing phases.

---

## ðŸ“Š Result Graphs Explained

### 1. `training_progress.png`
**Description:**
- Shows the agent's learning progress during training.
- **Top Left:** Episode scores (total reward per episode) and their moving average.
- **Top Right:** Epsilon (exploration rate) decay over episodes, showing how the agent shifts from exploration to exploitation.
- **Bottom Left:** Handover success rate per episode and its moving average, indicating how often handovers are successful.
- **Bottom Right:** System efficiency score per episode and its moving average, reflecting overall network performance.

**Purpose:**
- To visualize how the RL agent improves over time in terms of reward, decision quality, and system efficiency.

---

### 2. `detailed_performance_metrics.png` / `test_detailed_metrics.png`
**Description:**
- Plots detailed system metrics tracked during training or testing.
- Metrics may include: handover count, success rate, connectivity ratio, system efficiency, and more.

**Purpose:**
- To provide a comprehensive view of the agent's impact on various aspects of VANET performance.

---

### 3. `baseline_comparison.png` / `test_performance_comparison.png`
**Description:**
- Compares the RL agent's performance to baseline strategies (Random, Greedy, No Handover).
- Metrics compared: average reward, handover success rate, connectivity ratio, and percentage improvement over the best baseline.

**Purpose:**
- To demonstrate the advantage of RL-based handover over simple heuristic approaches.

---

### 4. `learning_progress.png`
**Description:**
- Focuses on the agent's learning curve, typically showing average reward or success rate over time.

**Purpose:**
- To highlight the agent's improvement and convergence during training.

---

## ðŸ¤– Baseline Strategies: Random & Greedy (Train & Test)

### Random Strategy
- **Meaning:** At each decision point, the agent selects an action (handover to any RSU or no handover) completely at random.
- **Purpose:** Serves as a lower-bound baseline, showing the performance of a system with no intelligence or policy.
- **Train/Test:**
  - **Train:** Not used for training the RL agent, but can be used to compare against the agent's performance.
  - **Test:** Used as a benchmark to evaluate how much better the RL agent performs compared to random decisions.

### Greedy Strategy
- **Meaning:** At each decision point, the agent always selects the RSU with the strongest signal (if within coverage), or chooses no handover if no RSU is suitable.
- **Purpose:** Represents a simple, signal-strength-based heuristic, commonly used in traditional handover systems.
- **Train/Test:**
  - **Train:** Not used for training the RL agent, but can be used to compare against the agent's performance.
  - **Test:** Used as a benchmark to evaluate the RL agent's advantage over a straightforward, non-learning approach.

---

## Summary Table
| Graph/File                        | What It Shows                                      |
|-----------------------------------|----------------------------------------------------|
| `training_progress.png`           | RL agent's learning and performance over episodes   |
| `detailed_performance_metrics.png`| System metrics during training                     |
| `baseline_comparison.png`         | RL vs. baseline strategies (training)              |
| `learning_progress.png`           | Learning curve (reward/success rate)               |
| `test_performance_comparison.png` | RL vs. baseline strategies (testing)               |
| `test_detailed_metrics.png`       | System metrics during testing                      |

---

If you have further questions about any graph or strategy, feel free to ask! 