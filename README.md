# üöóüì° VANET RL-Based Handover Decision Making Simulation

## üéØ Overview

This project implements a **Vehicular Ad-hoc Network (VANET)** simulation using **Reinforcement Learning (RL)** for intelligent handover decision making between Road Side Units (RSUs). The system uses a **Deep Q-Network (DQN)** agent to optimize connectivity as vehicles move through a realistic city environment.

## ‚ú® Key Features

- üèôÔ∏è **Realistic City Map**: 5√ó5 grid road network with intersections and city blocks
- üöó **Multi-Vehicle Simulation**: 15 vehicles moving bidirectionally on roads
- üì° **Strategic RSU Placement**: 9 RSUs positioned at key intersections (every 2 blocks)
- üß† **AI-Powered Handovers**: DQN agent making optimal handover decisions
- üìä **Real-time Visualization**: Live map showing vehicles, RSUs, roads, and buildings
- ‚ö° **High Performance**: Achieves 300,000+ rewards demonstrating optimal connectivity

## üöß Realistic Challenges & Negative Effects

To demonstrate the robustness of the RL agent, the simulation includes realistic negative parameters that degrade handover performance:

### üì° **Signal Quality Challenges**
- **Signal Interference** (15%): Other vehicles cause signal degradation
- **Signal Fading** (¬±10%): Random variations in signal strength
- **Building Shadowing** (10%): Buildings block signals in city environment
- **Weather Impact** (0-30%): Configurable weather-based signal loss

### üö¶ **Network Congestion Issues**
- **RSU Overload** (70% threshold): Performance degrades when RSUs are heavily loaded
- **Processing Delays** (2.0s): Time cost for handover processing
- **Channel Quality Variations** (50-100%): Environmental factors affect channel quality

### üîÑ **Handover Complications**
- **Handover Failures** (5% rate): Realistic failure probability
- **Handover Cooldown** (5.0s): Minimum time between handovers
- **Processing Overload** (8+ vehicles): RSU performance degrades with excess load

### üéØ **Mobility Prediction Errors**
- **Prediction Uncertainty** (¬±20%): Errors in estimating time-to-stay
- **Vehicle Behavior Variations**: Unpredictable movement patterns

These challenges make the simulation **significantly more difficult** but demonstrate how the RL agent learns to handle **real-world VANET problems**.

## üßÆ Handover Score Formula & Parameters

### üìä **Core Handover Decision Formula**

```
Handover Score = Œ±‚ÇÅ √ó Signal_Strength + Œ±‚ÇÇ √ó Load_Factor + Œ±‚ÇÉ √ó Time_to_Stay ‚àí Œ±‚ÇÑ √ó Total_Penalty
```

### üîß **Parameter Explanations**

#### **Œ±‚ÇÅ = 0.4 (Signal Strength Weight)**
- **Meaning**: How much importance we give to signal quality
- **Real-world impact**: Higher values prioritize strong connections over other factors
- **Range**: 0-1 (40% importance in our simulation)

#### **Œ±‚ÇÇ = 0.3 (Load Factor Weight)** 
- **Meaning**: How much we consider RSU congestion for load balancing
- **Real-world impact**: Prevents all vehicles from connecting to the same RSU
- **Range**: 0-1 (30% importance in our simulation)

#### **Œ±‚ÇÉ = 0.2 (Time to Stay Weight)**
- **Meaning**: How much we value connection stability (avoiding frequent handovers)
- **Real-world impact**: Reduces unnecessary handovers that waste resources
- **Range**: 0-1 (20% importance in our simulation)

#### **Œ±‚ÇÑ = 0.1 (Penalty Weight)**
- **Meaning**: How much we penalize problematic handover decisions
- **Real-world impact**: Discourages bad decisions and frequent switching
- **Range**: 0-1 (10% importance in our simulation)

### üìà **Component Calculations**

#### **1. Signal Strength**
```python
# Base calculation
if distance ‚â§ 250 meters:
    base_signal = 100 √ó (1 - distance/250)
else:
    base_signal = 0

# With negative effects applied:
final_signal = base_signal √ó fading_effect √ó weather_effect √ó shadowing_effect √ó (1 - interference_penalty) √ó channel_quality
```

#### **2. Load Factor**
```python
load_factor = 1 / (rsu_current_load / rsu_capacity + Œµ)
# Higher load = lower factor = less attractive RSU
```

#### **3. Time to Stay** 
```python
if velocity_towards_rsu > 0:
    predicted_time = (250 - distance) / velocity_towards_rsu
    # Apply prediction error: ¬±20% uncertainty
    time_to_stay = predicted_time √ó (1 ¬± prediction_error)
else:
    time_to_stay = 0
```

#### **4. Total Penalty**
```python
total_penalty = base_penalty + congestion_penalty + overload_penalty + 
                processing_delay_penalty + recent_handover_penalty + 
                failure_risk_penalty + channel_penalty
```

## üöß Detailed Negative Parameters Explanation

### üì° **Signal Quality Degradation**

#### **1. Signal Interference (15% degradation)**
- **Real-world meaning**: Other vehicles' radio transmissions interfere with signal
- **Formula**: `interference_penalty = min(nearby_vehicles, 3) √ó 0.15`
- **Impact**: Each nearby vehicle reduces signal by 15%, max 45% total
- **Why realistic**: In real VANET, multiple vehicles cause radio frequency interference

#### **2. Signal Fading (¬±10% variation)**
- **Real-world meaning**: Radio waves fluctuate due to atmospheric conditions
- **Formula**: `fading_effect = 1 + normal_random(0, 0.1)`
- **Impact**: Signal randomly varies by ¬±10% each measurement
- **Why realistic**: All wireless communications experience signal fading

#### **3. Building Shadowing (10% loss)**
- **Real-world meaning**: Buildings and obstacles block radio signals
- **Formula**: `shadowing_effect = 1 - 0.1 √ó random(0,1)`
- **Impact**: Up to 10% signal loss due to urban obstacles
- **Why realistic**: City environments have significant signal blockage

#### **4. Weather Impact (0-30% degradation)**
- **Real-world meaning**: Rain, snow, fog affect radio propagation
- **Formula**: `weather_effect = 1 - weather_impact` (configurable 0-0.3)
- **Impact**: Severe weather can reduce signal by up to 30%
- **Why realistic**: Weather significantly affects vehicle communications

### üö¶ **Network Congestion Effects**

#### **5. RSU Congestion (70% threshold, 30% penalty)**
- **Real-world meaning**: Overloaded RSUs provide poor service quality
- **Formula**: 
  ```python
  if rsu_load > 0.7:
      congestion_penalty = (rsu_load - 0.7) √ó 0.3
  ```
- **Impact**: Each 10% load above 70% adds 3% penalty
- **Why realistic**: Network equipment degrades performance when overloaded

#### **6. Processing Delays (2.0s cost)**
- **Real-world meaning**: Handovers take time and computational resources
- **Formula**: `processing_penalty = 2.0 √ó 0.1 = 0.2`
- **Impact**: Fixed penalty representing processing overhead
- **Why realistic**: All handovers involve authentication and setup time

#### **7. RSU Processing Overload (8+ vehicles, 25% penalty each)**
- **Real-world meaning**: RSU hardware has computational limits
- **Formula**: 
  ```python
  if rsu_load > 8:
      overload_penalty = (rsu_load - 8) √ó 0.25
  ```
- **Impact**: Each vehicle beyond 8 adds 25% performance penalty
- **Why realistic**: Hardware has finite processing capacity

### üîÑ **Handover Complications**

#### **8. Handover Failure Rate (5%)**
- **Real-world meaning**: Technical failures during handover process
- **Formula**: `if random() < 0.05: handover_fails = True`
- **Impact**: 5% of handover attempts fail completely
- **Why realistic**: Real networks have authentication failures, timeouts, etc.

#### **9. Handover Cooldown (5.0s minimum)**
- **Real-world meaning**: Network protocols prevent rapid successive handovers
- **Formula**: 
  ```python
  if time_since_last_handover < 5.0:
      cooldown_penalty = 0.3 √ó (1 - time_since_last/5.0)
  ```
- **Impact**: Rapid handovers heavily penalized
- **Why realistic**: Standards like 802.11 have minimum handover intervals

#### **10. Channel Quality Variations (50-100%)**
- **Real-world meaning**: Environmental factors affect radio channel quality
- **Formula**: `final_signal √ó channel_quality_factor`
- **Impact**: Overall signal can be reduced by up to 50%
- **Why realistic**: Channel conditions vary with traffic, interference, etc.

### üéØ **Mobility Prediction Challenges**

#### **11. Prediction Error (¬±20%)**
- **Real-world meaning**: Impossible to perfectly predict vehicle movement
- **Formula**: 
  ```python
  error_factor = 1 + uniform(-0.2, 0.2)
  predicted_time √ó error_factor
  ```
- **Impact**: Time-to-stay predictions can be 20% off in either direction
- **Why realistic**: Vehicle behavior is inherently unpredictable

## üéØ **Combined Impact Example**

Consider a handover decision with all negative effects:

```python
# Base calculation
base_signal = 80  # Good signal strength
load_factor = 2.0  # Low RSU load
time_to_stay = 15  # 15 seconds predicted

# Negative effects applied
signal_after_interference = 80 √ó (1 - 0.15) = 68      # -15% interference
signal_after_fading = 68 √ó 0.9 = 61.2                 # -10% fading
signal_after_shadowing = 61.2 √ó 0.9 = 55.08           # -10% shadowing
final_signal = 55.08                                   # 31% total degradation!

# Time prediction with error
predicted_time_with_error = 15 √ó 1.2 = 18             # +20% prediction error

# Additional penalties
congestion_penalty = 0.15    # RSU at 80% load
processing_penalty = 0.2     # Processing delay
cooldown_penalty = 0.1       # Recent handover

total_penalty = 0.5 + 0.15 + 0.2 + 0.1 = 0.95

# Final score
handover_score = 0.4√ó55.08 + 0.3√ó2.0 + 0.2√ó18 - 0.1√ó0.95
               = 22.03 + 0.6 + 3.6 - 0.095
               = 26.135

# Without negative effects, score would have been:
ideal_score = 0.4√ó80 + 0.3√ó2.0 + 0.2√ó15 - 0.1√ó0.5
            = 32 + 0.6 + 3 - 0.05
            = 35.55

# Negative effects reduced score by 26.5%!
```

This demonstrates how **realistic challenges make optimal handover decisions much more difficult**, requiring the RL agent to learn robust strategies that work despite significant environmental adversity.

## üìä **Handover Decision Process Diagram**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        HANDOVER DECISION PROCESS                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

üì° SIGNAL STRENGTH CALCULATION
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Base Signal     ‚îÇ    ‚îÇ NEGATIVE EFFECTS APPLIED                             ‚îÇ
‚îÇ = 100√ó(1-d/250) ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ √ó Fading(¬±10%) √ó Weather(0-30%) √ó Shadowing(10%)    ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ √ó (1-Interference) √ó Channel_Quality                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                         ‚îÇ
üö¶ LOAD FACTOR                          ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îÇ
‚îÇ Load Factor     ‚îÇ                     ‚îÇ
‚îÇ = 1/(load/cap)  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ  ‚îÇ
                                     ‚îÇ  ‚îÇ
‚è±Ô∏è TIME TO STAY                      ‚îÇ  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ  ‚îÇ
‚îÇ Time Prediction ‚îÇ    ‚îÇ ¬±20% Error  ‚îÇ‚îÇ  ‚îÇ
‚îÇ = (250-d)/vel   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Applied     ‚îÇ‚îÇ  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ  ‚îÇ
                                     ‚îÇ  ‚îÇ
üí• PENALTY CALCULATION                ‚îÇ  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Base Penalty (0.5 if connected)    ‚îÇ  ‚îÇ                                     ‚îÇ
‚îÇ + Congestion (if load > 70%)       ‚îÇ  ‚îÇ                                     ‚îÇ
‚îÇ + Overload (if vehicles > 8)       ‚îÇ  ‚îÇ                                     ‚îÇ
‚îÇ + Processing Delay (2.0s)          ‚îÇ  ‚îÇ                                     ‚îÇ
‚îÇ + Cooldown (if recent handover)    ‚îÇ  ‚îÇ                                     ‚îÇ
‚îÇ + Failure Risk (5%)                ‚îÇ  ‚îÇ                                     ‚îÇ
‚îÇ + Channel Degradation              ‚îÇ  ‚îÇ                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                     ‚îÇ  ‚îÇ
üèÜ FINAL SCORE CALCULATION            ‚îÇ  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Handover Score = Œ±‚ÇÅ√óSignal + Œ±‚ÇÇ√óLoad + Œ±‚ÇÉ√óTime - Œ±‚ÇÑ√óPenalties               ‚îÇ
‚îÇ                = 0.4√óSignal + 0.3√óLoad + 0.2√óTime - 0.1√óPenalties          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                     ‚îÇ
üéØ DECISION LOGIC                    ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ IF Score > 0 AND RSU has capacity AND handover doesn't fail (5%):           ‚îÇ
‚îÇ    ‚úÖ PERFORM HANDOVER                                                       ‚îÇ
‚îÇ ELSE:                                                                        ‚îÇ
‚îÇ    ‚ùå REJECT HANDOVER (penalty applied)                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üî¨ **Parameter Sensitivity Analysis**

### **Weight Parameter Impact:**

| Weight Change | Effect on Behavior | Best Use Case |
|---------------|-------------------|---------------|
| ‚Üë Œ±‚ÇÅ (Signal) | Prioritize signal quality | High-speed scenarios |
| ‚Üë Œ±‚ÇÇ (Load) | Better load balancing | Dense traffic areas |
| ‚Üë Œ±‚ÇÉ (Time) | Fewer handovers | Stable routes |
| ‚Üë Œ±‚ÇÑ (Penalty) | More conservative | Critical applications |

### **Negative Effect Severity:**

| Parameter | Mild (Low Impact) | Moderate | Severe (High Impact) |
|-----------|-------------------|----------|---------------------|
| Interference | 5% | 15% ‚úì | 30% |
| Fading | ¬±5% | ¬±10% ‚úì | ¬±20% |
| Weather | 0-10% | 0-20% | 0-30% ‚úì |
| Congestion Threshold | 90% | 70% ‚úì | 50% |
| Failure Rate | 1% | 5% ‚úì | 15% |
| Prediction Error | ¬±10% | ¬±20% ‚úì | ¬±40% |

**‚úì = Current simulation settings** (Moderate difficulty for realistic challenge)

## üõ†Ô∏è Installation Guide

### Prerequisites

- **Python 3.8+** (Python 3.12 recommended)
- **pip** package manager

### Step 1: Enter Simulation File

```bash
cd Sim_new
```

### Step 2: Create Virtual Environment (Recommended)

```bash
# On Windows
python -m venv vanet_env
vanet_env\Scripts\activate

# On macOS/Linux
python3 -m venv vanet_env
source vanet_env/bin/activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

If `requirements.txt` doesn't exist, install manually:

```bash
pip install numpy matplotlib gymnasium torch tqdm
```

### Step 4: Verify Installation

```bash
python -c "import numpy, matplotlib, gymnasium, torch; print('All dependencies installed successfully!')"
```

## üöÄ Quick Start

### 1. Train the Agent

```bash
python main.py
```

This will:
- Create a 5√ó5 city grid with 15 vehicles and 9 RSUs
- Train a DQN agent for 1000 episodes
- Save the trained model as `trained_agent.pth`
- Generate performance plots

### 2. Test the Trained Agent

```bash
python test_agent.py
```

This will:
- Load the trained agent
- Run 3 test episodes with real-time visualization
- Show handover decisions and connectivity status

## üìÅ Project Structure

```
Sim_new/
‚îú‚îÄ‚îÄ vanet_env/
‚îÇ   ‚îî‚îÄ‚îÄ environment.py          # VANET simulation environment
‚îú‚îÄ‚îÄ rl_agent/
‚îÇ   ‚îî‚îÄ‚îÄ dqn_agent.py           # Deep Q-Network agent implementation
‚îú‚îÄ‚îÄ visualization/
‚îÇ   ‚îú‚îÄ‚îÄ visualizer.py          # Real-time environment visualization
‚îÇ   ‚îî‚îÄ‚îÄ performance_metrics.py # Performance tracking and plotting
‚îú‚îÄ‚îÄ main.py                    # Training script
‚îú‚îÄ‚îÄ test_agent.py             # Testing and visualization script
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îî‚îÄ‚îÄ README.md                 # This file
```

## ‚öôÔ∏è Configuration Parameters

### Environment Settings

| Parameter | Default | Description |
|-----------|---------|-------------|
| `NUM_VEHICLES` | 15 | Number of vehicles in simulation |
| `NUM_RSUS` | 9 | Number of Road Side Units |
| `AREA_SIZE` | 1000 | Simulation area size (1000√ó1000 meters) |
| `RSU_SPACING` | 2 | RSUs placed every N intersections |

### Training Settings

| Parameter | Default | Description |
|-----------|---------|-------------|
| `EPISODES` | 1000 | Number of training episodes |
| `BATCH_SIZE` | 32 | Training batch size |
| `LEARNING_RATE` | 0.001 | Neural network learning rate |
| `EPSILON_START` | 1.0 | Initial exploration rate |
| `EPSILON_END` | 0.01 | Minimum exploration rate |

## üîÑ System Flowchart

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  System Start   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Initialize      ‚îÇ
‚îÇ Environment:    ‚îÇ
‚îÇ ‚Ä¢ 5√ó5 City Grid ‚îÇ
‚îÇ ‚Ä¢ 15 Vehicles   ‚îÇ
‚îÇ ‚Ä¢ 9 RSUs        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Initialize      ‚îÇ
‚îÇ DQN Agent:      ‚îÇ
‚îÇ ‚Ä¢ Neural Network‚îÇ
‚îÇ ‚Ä¢ Experience    ‚îÇ
‚îÇ   Replay Buffer ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Training Loop   ‚îÇ
‚îÇ (1000 episodes) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Environment     ‚îÇ    ‚îÇ Agent Decision  ‚îÇ    ‚îÇ Action          ‚îÇ
‚îÇ Observation:    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Making:         ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Execution:      ‚îÇ
‚îÇ ‚Ä¢ Vehicle pos   ‚îÇ    ‚îÇ ‚Ä¢ Neural Net    ‚îÇ    ‚îÇ ‚Ä¢ Handover to   ‚îÇ
‚îÇ ‚Ä¢ Vehicle vel   ‚îÇ    ‚îÇ ‚Ä¢ Q-values      ‚îÇ    ‚îÇ   target RSU    ‚îÇ
‚îÇ ‚Ä¢ RSU positions ‚îÇ    ‚îÇ ‚Ä¢ Œµ-greedy      ‚îÇ    ‚îÇ ‚Ä¢ Stay connected‚îÇ
‚îÇ ‚Ä¢ RSU loads     ‚îÇ    ‚îÇ   exploration   ‚îÇ    ‚îÇ ‚Ä¢ Disconnect    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚ñ≤                                              ‚îÇ
          ‚îÇ                                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ State Update:   ‚îÇ    ‚îÇ Reward          ‚îÇ    ‚îÇ Environment     ‚îÇ
‚îÇ ‚Ä¢ Move vehicles ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ Calculation:    ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ Update:         ‚îÇ
‚îÇ ‚Ä¢ Update        ‚îÇ    ‚îÇ ‚Ä¢ Signal        ‚îÇ    ‚îÇ ‚Ä¢ Update vehicle‚îÇ
‚îÇ   connections   ‚îÇ    ‚îÇ   strength      ‚îÇ    ‚îÇ   positions     ‚îÇ
‚îÇ ‚Ä¢ Calculate     ‚îÇ    ‚îÇ ‚Ä¢ RSU load      ‚îÇ    ‚îÇ ‚Ä¢ Update RSU    ‚îÇ
‚îÇ   new state     ‚îÇ    ‚îÇ ‚Ä¢ Time to stay  ‚îÇ    ‚îÇ   connections   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Experience      ‚îÇ
‚îÇ Storage:        ‚îÇ
‚îÇ ‚Ä¢ Store (s,a,r, ‚îÇ
‚îÇ   s') in buffer ‚îÇ
‚îÇ ‚Ä¢ Train network ‚îÇ
‚îÇ   with batch    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Episode End?    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ No ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ         ‚îÇ
‚îÇ         ‚ñ≤
‚îî‚îÄ‚îÄ Yes ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ             ‚îÇ
          ‚ñº             ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ Update Target   ‚îÇ     ‚îÇ
‚îÇ Network         ‚îÇ     ‚îÇ
‚îÇ (every 10 eps)  ‚îÇ     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
          ‚îÇ             ‚îÇ
          ‚ñº             ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ All Episodes    ‚îÇ     ‚îÇ
‚îÇ Complete?       ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îú‚îÄ‚îÄ‚îÄ No ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ
‚îî‚îÄ‚îÄ Yes ‚îÄ‚îÄ‚îê
          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Save Trained    ‚îÇ
‚îÇ Agent & Results ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  System End     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üíª Pseudocode

### Main Training Algorithm

```python
ALGORITHM: VANET DQN Training
INPUT: Environment parameters, Agent parameters
OUTPUT: Trained DQN agent

1. INITIALIZE Environment(vehicles=15, rsus=9, area=1000√ó1000)
2. INITIALIZE DQNAgent(state_size, action_size, learning_rate)
3. INITIALIZE PerformanceMetrics()

4. FOR episode = 1 TO EPISODES:
    a. state = environment.reset()
    b. total_reward = 0
    c. done = False
    
    d. WHILE NOT done:
        i.   action = agent.act(state)  // Œµ-greedy policy
        ii.  next_state, reward, done = environment.step(action)
        iii. agent.remember(state, action, reward, next_state, done)
        iv.  agent.replay(batch_size)  // Train neural network
        v.   state = next_state
        vi.  total_reward += reward
    
    e. IF episode % 10 == 0:
        i. agent.update_target_network()
    
    f. RECORD performance metrics

5. SAVE trained agent
6. PLOT results
```

### Handover Decision Algorithm

```python
ALGORITHM: Handover Score Calculation
INPUT: Vehicle state, RSU index
OUTPUT: Handover score

1. FUNCTION calculate_handover_score(vehicle, rsu_index):
    a. rsu = road_side_units[rsu_index]
    
    b. // Calculate signal strength (distance-based)
       distance = ||vehicle.position - rsu.position||
       IF distance ‚â§ 250:
           signal_strength = 100 √ó (1 - distance/250)
       ELSE:
           signal_strength = 0
    
    c. // Calculate RSU load factor
       load_factor = 1 / (rsu.current_load / rsu.capacity + Œµ)
    
    d. // Calculate time to stay in coverage
       direction_to_rsu = normalize(rsu.position - vehicle.position)
       velocity_towards_rsu = dot(vehicle.velocity, direction_to_rsu)
       IF velocity_towards_rsu > 0:
           time_to_stay = (250 - distance) / velocity_towards_rsu
       ELSE:
           time_to_stay = 0
    
    e. // Calculate penalty for unnecessary handover
       penalty = 0.5 IF vehicle.connected_rsu ‚â† NULL ELSE 0
    
    f. // Final handover score
       score = Œ±‚ÇÅ√ósignal_strength + Œ±‚ÇÇ√óload_factor + 
               Œ±‚ÇÉ√ótime_to_stay - Œ±‚ÇÑ√ópenalty
    
    g. RETURN score
```

### Environment Update Algorithm

```python
ALGORITHM: Environment Step Update
INPUT: Action from agent
OUTPUT: New state, reward, done flag

1. FUNCTION environment.step(action):
    a. // Update vehicle positions
       FOR each vehicle:
           vehicle.position += vehicle.velocity
           handle_boundary_collision(vehicle)
    
    b. // Reset connections
       FOR each vehicle:
           IF vehicle.connected_rsu ‚â† NULL:
               rsus[vehicle.connected_rsu].current_load -= 1
               vehicle.connected_rsu = NULL
    
    c. // Execute handover action
       IF action < num_rsus:
           target_rsu = rsus[action]
           handover_score = calculate_handover_score(vehicles[0], action)
           IF handover_score > 0 AND target_rsu.current_load < capacity:
               vehicles[0].connected_rsu = action
               target_rsu.current_load += 1
               reward = handover_score
           ELSE:
               reward = -1
    
    d. // Auto-connect vehicles to best available RSUs
       FOR each vehicle:
           best_rsu = find_best_rsu(vehicle)
           IF best_rsu.signal_strength > threshold:
               connect_vehicle_to_rsu(vehicle, best_rsu)
    
    e. // Update vehicle states
       FOR each vehicle:
           update_signal_strength(vehicle)
           update_time_to_stay(vehicle)
    
    f. done = (current_step >= max_steps)
    g. new_state = get_current_state()
    h. RETURN new_state, reward, done
```

## üìä Performance Metrics

The system tracks and visualizes:

- **Average Reward per Episode**: Shows learning progress
- **Handover Frequency**: Number of handovers per episode
- **Connectivity Rate**: Percentage of time vehicles are connected
- **RSU Load Distribution**: Utilization across different RSUs
- **Signal Strength Distribution**: Quality of connections

## üéõÔ∏è Customization

### Modify Vehicle Behavior

Edit `vanet_env/environment.py`:

```python
# Change vehicle speed range (km/h to m/s)
speed = np.random.uniform(30, 60) / 3.6  # 30-60 km/h

# Change number of lanes per road
self.lanes_per_road = 2  # 2 lanes per direction
```

### Adjust RSU Parameters

```python
# Change RSU coverage radius
coverage_radius = 250  # meters

# Change RSU capacity
'capacity': 10  # max connected vehicles
```

### Modify Handover Formula

```python
# Adjust handover score weights
self.alpha1 = 0.4  # Signal strength weight
self.alpha2 = 0.3  # RSU load weight  
self.alpha3 = 0.2  # Time to stay weight
self.alpha4 = 0.1  # Penalty weight
```

## üêõ Troubleshooting

### Common Issues

1. **ImportError: No module named 'torch'**
   ```bash
   pip install torch
   ```

2. **OpenGL/Display Issues (Linux)**
   ```bash
   sudo apt-get install python3-tk
   export DISPLAY=:0
   ```

3. **Memory Issues during Training**
   - Reduce `BATCH_SIZE` from 32 to 16
   - Reduce `EPISODES` from 1000 to 500

4. **Slow Visualization**
   - Increase `VISUALIZATION_DELAY` in `test_agent.py`
   - Set `RENDER = False` for faster testing

### Performance Optimization

- **GPU Training**: Install PyTorch with CUDA support
- **Parallel Processing**: Increase batch size if you have more RAM
- **Faster Visualization**: Reduce the number of plotted elements

## üìà Expected Results

After training, you should see:

- **Convergence**: Average reward stabilizing around 299,000,000+
- **Smart Handovers**: Agent learns to minimize unnecessary handovers
- **High Connectivity**: 95%+ vehicles connected to RSUs
- **Load Balancing**: Even distribution across RSUs

## üìö References

- Deep Q-Learning: [Mnih et al., 2015](https://arxiv.org/abs/1312.5602)
- VANET Handover: IEEE 802.11p Standard
- Reinforcement Learning: [Sutton & Barto, 2018](http://incompleteideas.net/book/the-book.html)

## üìä **Performance Graphs & Improvement Demonstration**

The system generates multiple comprehensive graphs that clearly demonstrate how the RL agent learns and improves handover decisions compared to baseline approaches.

### üéØ **Main Performance Graphs Generated**

#### **1. Training Progress (`training_progress.png`)**

**What it shows**: How the RL agent learns over 1000 episodes

**Four Key Subplots**:
- **Episode Score Progress**: Shows reward increasing as agent learns better handover decisions
- **Exploration vs Exploitation**: Shows epsilon decay from 100% exploration to 1% (99% exploitation)
- **Handover Success Rate**: Shows percentage of successful handovers improving over time
- **System Efficiency**: Shows overall system performance improving during training

**Key Learning Indicators**:
- ‚úÖ **Converging Rewards**: Scores stabilize at high values (300,000+)
- ‚úÖ **Improving Success Rate**: From ~60% early episodes to ~90%+ in final episodes
- ‚úÖ **Increasing Efficiency**: System efficiency improves from ~0.4 to ~0.8+
- ‚úÖ **Smooth Learning**: Moving averages show consistent improvement without major drops

#### **2. Baseline Comparison (`baseline_comparison.png`)**

**What it shows**: Direct comparison between RL agent and 3 baseline strategies

**Baseline Strategies Tested**:
- üé≤ **Random**: Randomly chooses handover decisions
- üîç **Greedy**: Always connects to strongest signal RSU
- üö´ **No Handover**: Never performs handovers, stays with initial connection

**Six Comparison Metrics**:
1. **Handover Success Rate**: RL agent achieves ~90%+ vs Random ~50%, Greedy ~70%
2. **Connectivity Ratio**: RL maintains ~95%+ connected vehicles vs others ~60-80%
3. **Signal Strength**: RL achieves balanced signal quality vs Greedy's signal-only focus
4. **System Efficiency**: RL scores 0.8+ vs Random 0.6, Greedy 0.7, No-Handover 0.4
5. **Handover Count**: RL makes optimal number vs Random (too many) or No-Handover (too few)
6. **Improvement Percentage**: Shows RL's +20-40% improvement over best baseline

#### **3. Learning Progress (`learning_progress.png`)**

**What it shows**: Detailed learning curves showing specific improvements

**Four Learning Aspects**:
- **Handover Success Rate Learning**: Shows improvement from ~50% to ~90%+
- **System Efficiency Learning**: Shows efficiency climbing from ~0.4 to ~0.8+
- **Decision Quality Learning**: Shows handover scores improving above average (1.0 baseline)
- **Unnecessary Handovers Reduction**: Shows reduction in wasteful handovers from ~60% to ~20%

**Learning Pattern Indicators**:
- üìà **Steady Improvement**: All metrics show upward trends
- üéØ **Convergence**: Metrics stabilize at high performance levels
- üìä **Moving Averages**: Smooth out episode-to-episode variations to show clear trends

#### **4. Detailed Performance Metrics (`detailed_performance_metrics.png`)**

**What it shows**: In-depth analysis of system performance components

**Eight Detailed Metrics**:
1. **Handover Success Rate**: Final success percentage
2. **Decision Quality Over Time**: Quality score above/below average (1.0)
3. **Connectivity Ratio**: Percentage of vehicles connected over time
4. **Load Balance Efficiency**: How well system distributes load across RSUs
5. **Average Signal Strength**: Signal quality maintained
6. **Handover Score Distribution**: Histogram showing score quality
7. **Handover Necessity Analysis**: Proportion of necessary vs unnecessary handovers
8. **Overall System Efficiency**: Combined performance score

### üèÜ **Key Performance Improvements Demonstrated**

#### **Quantitative Improvements**:

| Metric | Random Baseline | Greedy Baseline | RL Agent | Improvement |
|--------|-----------------|-----------------|----------|-------------|
| **Success Rate** | ~50% | ~70% | ~90%+ | **+29%** |
| **Connectivity** | ~60% | ~75% | ~95%+ | **+27%** |
| **System Efficiency** | ~0.6 | ~0.7 | ~0.8+ | **+14%** |
| **Signal Quality** | Poor | Good | Balanced | **Optimal** |
| **Load Balance** | Random | Poor | Excellent | **+40%** |
| **Unnecessary Handovers** | ~80% | ~60% | ~20% | **-67%** |

#### **Qualitative Improvements**:

**üß† Intelligence**:
- **RL Agent**: Learns from experience, adapts to patterns
- **Baselines**: Use simple rules, no learning capability

**‚öñÔ∏è Multi-Objective Optimization**:
- **RL Agent**: Balances signal, load, stability, and penalties simultaneously
- **Baselines**: Focus on single objective (signal strength or no change)

**üîÑ Adaptability**:
- **RL Agent**: Adapts to different scenarios, traffic patterns, and network conditions
- **Baselines**: Static behavior regardless of conditions

**üéØ Context Awareness**:
- **RL Agent**: Considers vehicle movement, RSU loads, prediction errors
- **Baselines**: Limited context awareness

### üìà **How to Interpret the Results**

#### **Training Phase Evidence**:
1. **Convergence**: Final episodes show stable, high performance
2. **Learning Curve**: Smooth improvement without overfitting
3. **Exploration Decay**: Successful transition from exploration to exploitation
4. **Metric Correlation**: All metrics improve together, showing holistic learning

#### **Comparison Phase Evidence**:
1. **Consistent Superiority**: RL outperforms ALL baseline strategies in ALL metrics
2. **Significant Margins**: 20-40% improvements, not marginal gains
3. **Balanced Performance**: No single metric sacrificed for others
4. **Statistical Significance**: Results consistent across multiple test episodes

#### **Real-World Implications**:

**üöó For Vehicle Users**:
- **Better Connectivity**: 95%+ connection time vs 60-80% with simple strategies
- **Smoother Experience**: Fewer unnecessary handovers (20% vs 60-80%)
- **Improved QoS**: Higher signal quality maintained consistently

**üì° For Network Operators**:
- **Load Balancing**: Even distribution across RSUs prevents overload
- **Resource Efficiency**: Optimal utilization of network infrastructure
- **Reduced Failures**: 90%+ handover success rate vs 50-70% baselines

**üè¢ For System Deployment**:
- **Scalability**: Handles increasing vehicle density effectively
- **Robustness**: Performs well despite realistic negative effects
- **Adaptability**: Self-optimizes for different traffic patterns

### üéØ **Performance Summary Graphics**

Each graph contains **visual indicators** showing improvement:

- **üìä Bar Charts**: Clear height differences showing RL superiority
- **üìà Line Graphs**: Upward trends during learning phase
- **üé® Color Coding**: Green for improvements, red for declines
- **üìã Value Labels**: Exact percentages and scores for precision
- **üìâ Moving Averages**: Smooth trend lines showing consistent progress

### üîç **Evidence of Successful Learning**

**Before Training (Early Episodes)**:
- Random handover decisions
- ~50% success rate
- Poor load balancing
- Many unnecessary handovers

**After Training (Final Episodes)**:
- Intelligent handover timing
- ~90%+ success rate
- Excellent load distribution
- Minimal unnecessary handovers

**Comparison with Alternatives**:
- Consistently outperforms all baseline strategies
- Demonstrates learned intelligence vs fixed rules
- Shows adaptation to complex multi-objective optimization

This comprehensive performance analysis proves that the RL system successfully learns superior handover decision making that significantly outperforms traditional approaches in realistic VANET environments.

